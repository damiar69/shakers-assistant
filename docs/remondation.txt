Guardar la nueva interacción en la base de datos
El usuario envía una petición a POST /rag/query con:

json
Copiar
Editar
{ "user_id": "usuario123", "query": "¿Cómo funcionan los pagos?" }
El backend:

Vectoriza la consulta, obtiene fragmentos relevantes, genera la respuesta con el LLM y recopila las referencias (por ejemplo, ["payments.md", "pricing.md"]).

Llama a la función add_chat_entry(user_id, pregunta, respuesta, referencias) y se crea una fila en SQLite:

id	user_id	question	answer	references
1	usuario123	¿Cómo funcionan los pagos?	“Los pagos en Shakers se realizan…”	payments.md,pricing.md

Esa fila ya forma parte de su “historial de consultas”.

2. Cálculo de recomendaciones con el historial actualizado
Cuando el usuario llama a POST /recs/personalized (por ejemplo, justo después de la pregunta RAG), el backend realizará lo siguiente:

Recuperar el historial completo

Se llama a get_user_history("usuario123"). Esto hace un SELECT * FROM ChatEntry WHERE user_id="usuario123" ORDER BY id.

Si ya hay más de una interacción, el arreglo podría verse así:

python
Copiar
Editar
[
  ChatEntry(id=1, user_id="usuario123",
            question="¿Cómo funcionan los pagos?",
            answer="Los pagos...", references="payments.md,pricing.md"),
  ChatEntry(id=2, user_id="usuario123",
            question="¿Cómo me registro?",
            answer="Para registrarte...", references="getting_started.md")
  # … y así hasta la interacción más reciente
]
Construir chat_history como lista de diccionarios

Se recorre cada fila y se convierte en un diccionario con estas claves:

python
Copiar
Editar
chat_history = [
  {"q": "¿Cómo funcionan los pagos?",
   "a": "Los pagos…",
   "refs": ["payments.md", "pricing.md"]},
  {"q": "¿Cómo me registro?",
   "a": "Para registrarte…",
   "refs": ["getting_started.md"]},
  # … más entradas según existan
]
Extraer el perfil del usuario (embedding promedio)

Para cada entrada en chat_history, tomamos su lista de referencias (refs) y las agregamos a un conjunto seen_docs.

python
Copiar
Editar
seen_docs = {"payments.md", "pricing.md", "getting_started.md"}
Luego, para cada nombre de documento en seen_docs, cargamos su embedding (ya guardado en doc_embeddings.json como array NumPy).

python
Copiar
Editar
embeddings_vistos = [
  DOC_EMBEDDINGS["payments.md"],
  DOC_EMBEDDINGS["pricing.md"],
  DOC_EMBEDDINGS["getting_started.md"]
]
Calculamos el promedio (vector medio) de todos esos embeddings:

python
Copiar
Editar
user_emb = np.mean(np.stack(embeddings_vistos), axis=0)
Ese user_emb representa el “perfil semántico” de los intereses del usuario:

Si consultó primero sobre pagos y luego sobre “cómo registrarse”, su embedding medio reflejará ambos temas.

Comparación con documentos no vistos

Recorremos todos los documentos en doc_embeddings.json. Supongamos que la base de conocimiento tiene estos archivos:

css
Copiar
Editar
["payments.md", "pricing.md", "getting_started.md", "refunds.md", "features.md", "security.md"]
Filtramos los que no están en seen_docs. En este caso, quedan:

ini
Copiar
Editar
candidatos = ["refunds.md", "features.md", "security.md"]
Para cada documento candidato:

Obtenemos su embedding (por ejemplo, emb_refunds, emb_features, emb_security).

Calculamos la similitud de coseno con user_emb:

python
Copiar
Editar
sim_refunds   = cosine_similarity(user_emb, emb_refunds)
sim_features  = cosine_similarity(user_emb, emb_features)
sim_security  = cosine_similarity(user_emb, emb_security)
Ahora tenemos una lista de tuplas [(“refunds.md”, 0.82), (“features.md”, 0.65), (“security.md”, 0.40)].

Ordenar por similitud y escoger top-k con variedad

Ordenamos descendente por el puntaje de similitud:

css
Copiar
Editar
[("refunds.md", 0.82), ("features.md", 0.65), ("security.md", 0.40)]
Definimos k = 3 (queremos hasta 3 recomendaciones).

Para garantizar diversidad, normalmente agrupamos por un “prefijo” en el nombre (p. ej., todo lo que antes del guion bajo _). En este ejemplo los archivos no comparten prefijos (“refunds”, “features”, “security” son todos distintos), así que podemos recomendar los tres en orden.

Si hubiéramos tenido dos archivos con el mismo prefijo (por ejemplo, “payments_methods.md” y “payments_guidelines.md”), solo elegiríamos uno de ellos para no saturar al usuario con el mismo tema. En su lugar pasaríamos al siguiente candidato diferente.

Construir la lista final de recomendaciones

Para cada doc seleccionado, creamos un motivo (“reason”) explicando la similitud:

python
Copiar
Editar
recommendations = [
  {
    "doc": "refunds.md",
    "reason": "Este documento ‘refunds.md’ es semánticamente similar (score=0.82) a tus intereses previos."
  },
  {
    "doc": "features.md",
    "reason": "Este documento ‘features.md’ es semánticamente similar (score=0.65) a tus intereses previos."
  },
  {
    "doc": "security.md",
    "reason": "Este documento ‘security.md’ es semánticamente similar (score=0.40) a tus intereses previos."
  }
]
Devolvemos esta lista como respuesta JSON de POST /recs/personalized.